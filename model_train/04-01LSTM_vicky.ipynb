{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93fb5d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arrow\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "# TODO:[-] 25-06-03 keras.src 路径是 TensorFlow 2.11 及更高版本中集成在 TensorFlow 内部的 Keras 3 中使用的。\n",
    "# from keras.src.layers import LSTM, Dropout, Bidirectional, Dense, Masking\n",
    "from keras.layers import LSTM, Dropout, Bidirectional, Dense, Masking\n",
    "from pandas import DatetimeIndex\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# from tensorflow.keras.losses import MeanSquaredError\n",
    "# 可视化结果（如果需要）\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pathlib\n",
    "import xml.etree.ElementTree as ET\n",
    "import xarray as xr\n",
    "import codecs\n",
    "import datetime\n",
    "\n",
    "# 先从海浪数据中提取出经纬度，时间，风，海浪高度\n",
    "# 解析单个文件，并存于字典内\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "# from utils import rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9be4e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_forecast.shape: (61, 732)\n",
      "df_realdata.shape: (61, 732)\n",
      "Epoch 1/10\n",
      "37/37 [==============================] - 156s 3s/step - loss: 0.0398 - val_loss: 0.0275\n",
      "Epoch 2/10\n",
      "37/37 [==============================] - 74s 2s/step - loss: 0.0268 - val_loss: 0.0319\n",
      "Epoch 3/10\n",
      "37/37 [==============================] - 64s 2s/step - loss: 0.0261 - val_loss: 0.0274\n",
      "Epoch 4/10\n",
      "37/37 [==============================] - 82s 2s/step - loss: 0.0250 - val_loss: 0.0266\n",
      "Epoch 5/10\n",
      "37/37 [==============================] - 51s 1s/step - loss: 0.0245 - val_loss: 0.0252\n",
      "Epoch 6/10\n",
      "37/37 [==============================] - 60s 2s/step - loss: 0.0242 - val_loss: 0.0275\n",
      "Epoch 7/10\n",
      "37/37 [==============================] - 62s 2s/step - loss: 0.0245 - val_loss: 0.0252\n",
      "Epoch 8/10\n",
      "37/37 [==============================] - 112s 3s/step - loss: 0.0235 - val_loss: 0.0255\n",
      "Epoch 9/10\n",
      "37/37 [==============================] - 141s 4s/step - loss: 0.0232 - val_loss: 0.0247\n",
      "Epoch 10/10\n",
      "37/37 [==============================] - 111s 3s/step - loss: 0.0231 - val_loss: 0.0259\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def to_do():\n",
    "    \"\"\"\n",
    "\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # vals\n",
    "    # mac\n",
    "    # forecast_path: str = r'/Volumes/DATA/01TRAINNING_DATA/WIND/01/df_ws_forecast.csv'\n",
    "    # realdata_path: str = r'/Volumes/DATA/FUB/MF01001/2024_local_df_utc_filter.csv'\n",
    "    # win\n",
    "    # forecast_path: str = r'Z:\\01TRAINNING_DATA\\standard\\df_ws_forecast.csv'\n",
    "    # realdata_path: str = r'Z:\\01TRAINNING_DATA\\standard\\2024_local_df_utc_filter.csv'\n",
    "    # model_path: str = r'G:\\05DATA\\02MODELS\\fit_model_250603.h5'\n",
    "    # razer\n",
    "    # TODO:[-] 25-06-08 新加入的razer配置\n",
    "    forecast_path: str = r'Z:\\SOURCE_MERGE_DATA\\df_ws_forecast.csv'\n",
    "    realdata_path: str = r'Z:\\SOURCE_MERGE_DATA\\2024_local_df_utc_183_split.csv'\n",
    "    model_path: str = r'E:\\05DATA\\fit_model_v2_250614_vicky.h5'\n",
    "    scaler_forecast: str = r'Z:\\01TRAINNING_DATA\\scaler\\scaler_forecast_250609.sav'\n",
    "    scaler_realdata: str = r'Z:\\01TRAINNING_DATA\\scaler\\scaler_realdata_250609.sav'\n",
    "\n",
    "    # step1: 加载标准化后的 预报 | 实况 数据集\n",
    "    # shape: (61,732)\n",
    "    df_forecast = pd.read_csv(forecast_path, encoding='utf-8', index_col=0)\n",
    "    # shape:(61,731)\n",
    "    df_realdata = pd.read_csv(realdata_path, encoding='utf-8', index_col=0)\n",
    "    # 实况拼接有问题需要手动去掉最后一列\n",
    "    # 使用按3小时进行分割的数据不需要去掉最后一列，这样 realdata 与 forecast 的 columns 一致\n",
    "    # df_realdata = df_realdata.drop(df_realdata.columns[-1], axis=1)\n",
    "    df_forecast = df_forecast.iloc[:61, :]\n",
    "    print(f'df_forecast.shape: {df_forecast.shape}')\n",
    "    print(f'df_realdata.shape: {df_realdata.shape}')\n",
    "    pass\n",
    "    # step2: 由于数据中存在nan，如何处理nan\n",
    "    pass\n",
    "    # step3: 数据标准化(提出nan值)\n",
    "    rows: int = df_forecast.shape[0]\n",
    "    cols: int = df_forecast.shape[1]\n",
    "    # TODO:[-] 25-05-28 注意原始数据中: forecast (61,732), real (61,732)\n",
    "    X = df_forecast.values.T.reshape(cols, rows, 1)\n",
    "    # TODO:[*] 25-05-11 注意 y 中有存在 nan\n",
    "    # ValueError: cannot reshape array of size 52776 into shape (732,72,1)\n",
    "    y = df_realdata.values.T.reshape(cols, rows, 1)\n",
    "\n",
    "    # step3-2:对数据进行归一化\n",
    "    # 拍扁数据为二维数组（n*timesteps, feature）进行归一化\n",
    "    X_flat = X.reshape(-1, 1)\n",
    "    y_flat = y.reshape(-1, 1)\n",
    "\n",
    "    # 分别为 X 和 y 定义归一化器（当然如果两者量纲一致，可用同一个 scaler）\n",
    "    scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    X_scaled = scaler_X.fit_transform(X_flat)\n",
    "    y_scaled = scaler_y.fit_transform(y_flat)\n",
    "    # TODO:[-] 25-06-09 保存归一化器\n",
    "    joblib.dump(scaler_X, scaler_forecast)\n",
    "    joblib.dump(scaler_y, scaler_realdata)\n",
    "\n",
    "    # 将归一化后的二维数据恢复为原来的3D形状\n",
    "    X = X_scaled.reshape(X.shape)\n",
    "    y = y_scaled.reshape(y.shape)\n",
    "\n",
    "    # step3-3: 对数据集进行划分\n",
    "    # 拆分数据集为训练集和测试集\n",
    "    # 此处 *_tran 均为 (585,72,1) | *_test 均为 (147,72,1)\n",
    "    # X_* 相当于是 预报数据集 | y_* 是实况(验证)数据集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    # 此处加入转换\n",
    "\n",
    "    X_train = np.array(X_train, dtype=np.float32)\n",
    "    X_test = np.array(X_test, dtype=np.float32)\n",
    "    y_train = np.array(y_train, dtype=np.float32)\n",
    "    y_test = np.array(y_test, dtype=np.float32)\n",
    "\n",
    "    # step4: 构建模型\n",
    "    model = Sequential()\n",
    "    X_train = np.nan_to_num(X_train, nan=0.0)\n",
    "    X_test = np.nan_to_num(X_test, nan=0.0)\n",
    "    y_train = np.nan_to_num(y_train, nan=0.0)\n",
    "    y_test = np.nan_to_num(y_test, nan=0.0)\n",
    "    \"\"\"\n",
    "        错误原因:\n",
    "            你在 Masking 层中设置了 input_shape=(25, 1)，这表示你期望输入数据的每个样本有 25 个时间步，每个时间步有 1 个特征。\n",
    "            然而，错误信息 found shape=(None, 72, 1) 清楚地表明，你的实际输入数据 X_train 和 X_test 的每个样本有 72 个时间步，而不是 25 个。\n",
    "            TODO:[-] 25-06-0 此处做了修改由于数据为 (61,732) 故 72 => 61\n",
    "\n",
    "    \"\"\"\n",
    "    # 模型构建的步骤:\n",
    "    # step1: 添加一个 Masking 层，指定输入中值为 0.0 的时刻将被“屏蔽”，即这些时间步不会对后续层产生影响。\n",
    "    # TODO:[-] 25-06-12 屏蔽是会去掉该时刻的所有数据吗？\n",
    "    model.add(Masking(mask_value=0.0, input_shape=(61, 1)))\n",
    "    # step2: 添加双向LSTM层\n",
    "    # 双向 LSTM 同时从前向和后向处理时序数据，从而捕获更多上下文信息，提升特征提取能力。\n",
    "    # units=128：LSTM 层中每个方向上有 128 个神经元。\n",
    "    # return_sequences=True：输出每个时间步的结果，而非仅仅输出最后时刻的状态，这样可以将整个序列的信息传递到下一层。\n",
    "    # activation='relu'：将激活函数设置为 ReLU（而非 LSTM 默认的 tanh），可能有助于缓解梯度消失问题，不过这取决于具体任务。\n",
    "    # 注意：虽然此处指定了 input_shape=(25, 1)，但实际上在 Sequential 模型中第一层已经指定了输入形状，所以这里的 input_shape 参数可能是不必要或引起混淆（建议保持与 Masking 层一致，即 (61, 1)）。\n",
    "    # v1 激活函数:relu\n",
    "    # v2 改为: tanh\n",
    "    model.add(Bidirectional(LSTM(units=256, return_sequences=True,\n",
    "                                 activation='tanh',\n",
    "                                 input_shape=(61, 1))))\n",
    "    # step3: 添加 Dropout 层，在训练时随机将 20% 的神经元输出设为 0。\n",
    "    # Dropout 是一种正则化方法，有助于防止模型过拟合；通过随机丢弃部分神经元，模型不能过分依赖局部特征，从而提高泛化能力。\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Bidirectional(LSTM(units=128, return_sequences=True, activation='tanh')))  # 可以堆叠多个LSTM层\n",
    "    # step5: 再次添加一个 Dropout 层，使得第二层 LSTM 的输出在训练时有 20% 被随机置零，从而进一步防止过拟合。\n",
    "    model.add(Dropout(0.2))\n",
    "    # step4: 第二层双向LSTM层\n",
    "    model.add(Bidirectional(LSTM(units=64, return_sequences=True, activation='tanh')))  # 可以堆叠多个LSTM层\n",
    "    # step5: 再次添加一个 Dropout 层，使得第二层 LSTM 的输出在训练时有 20% 被随机置零，从而进一步防止过拟合。\n",
    "    model.add(Dropout(0.2))\n",
    "    # step6: 添加全连接层（Dense 层），输出节点数为 25。25-06-12 此处修改为 61 ，需要预测长度为61的时间向量\n",
    "    # Dense 层将前面的时序特征映射到目标空间。在时序网络中，当上层返回整个序列（形状为 [batch_size, time_steps, features]）时，Dense 层会被逐时步地应用，输出每个时间步对应一个长度为 25 的向量。这通常用于预测任务，比如多步预测或者每个时刻有多个目标值的任务。\n",
    "    model.add(Dense(61))\n",
    "\n",
    "    # 将均方误差修改为均方根误差后\n",
    "    # step7: 编译模型\n",
    "    # optimizer='adam' 使用 Adam 优化器进行梯度下降更新。Adam 优化器能够自适应调整各参数的学习率，通常能较快收敛，并且对超参数设定不太敏感，不同于传统的 SGD。\n",
    "    # TODO:[*] 25-06-12 什么是超参数？\n",
    "    # loss='mse' 将损失函数设为均方误差（Mean Squared Error），这是回归问题常用的误差度量指标，模型训练的目标就是尽可能使预测值与真实值之间的均方误差最小化。\n",
    "    # 整体作用 model.compile() 会对模型进行配置，指定训练时用哪个优化器、用哪个损失函数，如果需要，还可以添加额外的评估指标。编译过程会为模型建立必要的计算图，并对参数进行初始化。\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # step8: 训练模型\n",
    "    # X_train, y_train 分别为训练数据和对应目标值。模型将以这些数据为依据不断调整参数，使预测值与真实目标值之间的 MSE 最小化。\n",
    "    # epochs=10 指定训练过程需要遍历整个训练集 10 次。每个 epoch 内部数据会根据 batch 大小分批更新参数。\n",
    "    # batch_size=16 每个训练步骤（step）使用 16 个样本进行梯度计算和模型更新。较小的 batch size 有助于捕捉更多细微变化，但训练时间可能更长；较大的 batch size 则计算稳定但可能导致泛化性下降。\n",
    "    # validation_data=(X_test, y_test) 在每个 epoch 结束后，模型也会评估一次在验证集上的损失。这有助于监控过拟合情况以及训练过程的稳定性。\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_test, y_test))\n",
    "    model.save(model_path)\n",
    "\n",
    "    pass\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "def main():\n",
    "    to_do()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc18244a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_forecast.shape: (61, 732)\n",
      "df_realdata.shape: (61, 732)\n",
      "Epoch 1/10\n",
      "37/37 [==============================] - 102s 2s/step - loss: 0.4651 - val_loss: 0.4204\n",
      "Epoch 2/10\n",
      "37/37 [==============================] - 69s 2s/step - loss: 0.3833 - val_loss: 0.3721\n",
      "Epoch 3/10\n",
      "37/37 [==============================] - 73s 2s/step - loss: 0.3664 - val_loss: 0.3707\n",
      "Epoch 4/10\n",
      "37/37 [==============================] - 71s 2s/step - loss: 0.3618 - val_loss: 0.3608\n",
      "Epoch 5/10\n",
      "37/37 [==============================] - 67s 2s/step - loss: 0.3546 - val_loss: 0.3487\n",
      "Epoch 6/10\n",
      "37/37 [==============================] - 77s 2s/step - loss: 0.3508 - val_loss: 0.3535\n",
      "Epoch 7/10\n",
      "37/37 [==============================] - 73s 2s/step - loss: 0.3498 - val_loss: 0.3478\n",
      "Epoch 8/10\n",
      "37/37 [==============================] - 73s 2s/step - loss: 0.3374 - val_loss: 0.3385\n",
      "Epoch 9/10\n",
      "37/37 [==============================] - 72s 2s/step - loss: 0.3336 - val_loss: 0.3535\n",
      "Epoch 10/10\n",
      "37/37 [==============================] - 73s 2s/step - loss: 0.3283 - val_loss: 0.3296\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def to_do():\n",
    "    \"\"\"\n",
    "\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # vals\n",
    "    # mac\n",
    "    # forecast_path: str = r'/Volumes/DATA/01TRAINNING_DATA/WIND/01/df_ws_forecast.csv'\n",
    "    # realdata_path: str = r'/Volumes/DATA/FUB/MF01001/2024_local_df_utc_filter.csv'\n",
    "    # win\n",
    "    # forecast_path: str = r'Z:\\01TRAINNING_DATA\\standard\\df_ws_forecast.csv'\n",
    "    # realdata_path: str = r'Z:\\01TRAINNING_DATA\\standard\\2024_local_df_utc_filter.csv'\n",
    "    # model_path: str = r'G:\\05DATA\\02MODELS\\fit_model_250603.h5'\n",
    "    # razer\n",
    "    # TODO:[-] 25-06-08 新加入的razer配置\n",
    "    forecast_path: str = r'Z:\\SOURCE_MERGE_DATA\\df_ws_forecast.csv'\n",
    "    realdata_path: str = r'Z:\\SOURCE_MERGE_DATA\\2024_local_df_utc_183_split.csv'\n",
    "    model_path: str = r'E:\\05DATA\\fit_model_v2_250614_vicky.h5'\n",
    "    scaler_forecast: str = r'Z:\\01TRAINNING_DATA\\scaler\\scaler_forecast_250618_vicky.sav'\n",
    "    scaler_realdata: str = r'Z:\\01TRAINNING_DATA\\scaler\\scaler_realdata_250618_vicky.sav'\n",
    "\n",
    "    # step1: 加载标准化后的 预报 | 实况 数据集\n",
    "    # shape: (61,732)\n",
    "    df_forecast = pd.read_csv(forecast_path, encoding='utf-8', index_col=0)\n",
    "    # shape:(61,731)\n",
    "    df_realdata = pd.read_csv(realdata_path, encoding='utf-8', index_col=0)\n",
    "    # 实况拼接有问题需要手动去掉最后一列\n",
    "    # 使用按3小时进行分割的数据不需要去掉最后一列，这样 realdata 与 forecast 的 columns 一致\n",
    "    # df_realdata = df_realdata.drop(df_realdata.columns[-1], axis=1)\n",
    "    df_forecast = df_forecast.iloc[:61, :]\n",
    "    print(f'df_forecast.shape: {df_forecast.shape}')\n",
    "    print(f'df_realdata.shape: {df_realdata.shape}')\n",
    "    pass\n",
    "    # step2: 由于数据中存在nan，如何处理nan\n",
    "    pass\n",
    "    # step3: 数据标准化(提出nan值)\n",
    "    rows: int = df_forecast.shape[0]\n",
    "    cols: int = df_forecast.shape[1]\n",
    "    # TODO:[-] 25-05-28 注意原始数据中: forecast (61,732), real (61,732)\n",
    "    X = df_forecast.values.T.reshape(cols, rows, 1)\n",
    "    # TODO:[*] 25-05-11 注意 y 中有存在 nan\n",
    "    # ValueError: cannot reshape array of size 52776 into shape (732,72,1)\n",
    "    y = df_realdata.values.T.reshape(cols, rows, 1)\n",
    "\n",
    "    # step3-2:对数据进行归一化\n",
    "    # 拍扁数据为二维数组（n*timesteps, feature）进行归一化\n",
    "    X_flat = X.reshape(-1, 1)\n",
    "    y_flat = y.reshape(-1, 1)\n",
    "\n",
    "    # 分别为 X 和 y 定义归一化器（当然如果两者量纲一致，可用同一个 scaler）\n",
    "    scaler_X = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler_y = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "    X_scaled = scaler_X.fit_transform(X_flat)\n",
    "    y_scaled = scaler_y.fit_transform(y_flat)\n",
    "    # TODO:[-] 25-06-09 保存归一化器\n",
    "    joblib.dump(scaler_X, scaler_forecast)\n",
    "    joblib.dump(scaler_y, scaler_realdata)\n",
    "    \n",
    "    # 1. 数据预处理\n",
    "    def create_sequences(data_x, date_y, seq_length, pred_length=1):\n",
    "        \"\"\"\n",
    "        创建时间序列样本\n",
    "        \n",
    "        参数:\n",
    "        data: 原始数据\n",
    "        seq_length: 输入序列长度\n",
    "        pred_length: 预测序列长度\n",
    "        \"\"\"\n",
    "        # 创建序列\n",
    "        X, y = [], []\n",
    "        for i in range(len(X_scaled) - seq_length - pred_length + 1):\n",
    "            X.append(X_scaled[i:i+seq_length])\n",
    "            y.append(y_scaled[i+seq_length:i+seq_length+pred_length])\n",
    "            return np.array(X), np.array(y)\n",
    "     \n",
    "     \n",
    "    seq_length = 20  # 根据前面的原则选择\n",
    "    pred_length = 61  # 预测未来7个时间步\n",
    "    # X, y = create_sequences(X_scaled,y_scaled, seq_length, pred_length)\n",
    "    \n",
    "\n",
    "    # 将归一化后的二维数据恢复为原来的3D形状\n",
    "    X = X_scaled.reshape(X.shape)\n",
    "    y = y_scaled.reshape(y.shape)\n",
    "\n",
    "    # step3-3: 对数据集进行划分\n",
    "    # 拆分数据集为训练集和测试集\n",
    "    # 此处 *_tran 均为 (585,72,1) | *_test 均为 (147,72,1)\n",
    "    # X_* 相当于是 预报数据集 | y_* 是实况(验证)数据集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    # 此处加入转换\n",
    "\n",
    "    X_train = np.array(X_train, dtype=np.float32)\n",
    "    X_test = np.array(X_test, dtype=np.float32)\n",
    "    y_train = np.array(y_train, dtype=np.float32)\n",
    "    y_test = np.array(y_test, dtype=np.float32)\n",
    "\n",
    "    # step4: 构建模型\n",
    "    model = Sequential()\n",
    "    X_train = np.nan_to_num(X_train, nan=0.0)\n",
    "    X_test = np.nan_to_num(X_test, nan=0.0)\n",
    "    y_train = np.nan_to_num(y_train, nan=0.0)\n",
    "    y_test = np.nan_to_num(y_test, nan=0.0)\n",
    "\n",
    "    \"\"\"\n",
    "        错误原因:\n",
    "            你在 Masking 层中设置了 input_shape=(25, 1)，这表示你期望输入数据的每个样本有 25 个时间步，每个时间步有 1 个特征。\n",
    "            然而，错误信息 found shape=(None, 72, 1) 清楚地表明，你的实际输入数据 X_train 和 X_test 的每个样本有 72 个时间步，而不是 25 个。\n",
    "            TODO:[-] 25-06-0 此处做了修改由于数据为 (61,732) 故 72 => 61\n",
    "\n",
    "    \"\"\"\n",
    "    #给模型加权重\n",
    "    # 1. 自定义加权损失函数\n",
    "    from tensorflow.keras import backend as K\n",
    "    def weighted_mse(y_true, y_pred, weight_array=None):\n",
    "        \"\"\"\n",
    "        加权均方误差损失函数\n",
    "        \n",
    "        参数:\n",
    "        y_true: 真实值\n",
    "        y_pred: 预测值\n",
    "        weight_array: 权重数组，长度应与预测时间步一致\n",
    "        \"\"\"\n",
    "        if weight_array is None:\n",
    "            # 默认权重：前20个时间步权重为2，其余为1\n",
    "            weight_array = np.ones(61)\n",
    "            weight_array[:5] = 5.0\n",
    "            weight_array[:1] = 10.0\n",
    "        \n",
    "        # 转换为tensor\n",
    "        weight_tensor = K.constant(weight_array, dtype=tf.float32)\n",
    "        \n",
    "        # 计算加权MSE\n",
    "        error = y_true - y_pred\n",
    "        weighted_error = error * weight_tensor\n",
    "        weighted_mse = K.mean(weighted_error ** 2)\n",
    "        \n",
    "        return weighted_mse\n",
    "    \n",
    "    # 模型构建的步骤:\n",
    "    # step1: 添加一个 Masking 层，指定输入中值为 0.0 的时刻将被“屏蔽”，即这些时间步不会对后续层产生影响。\n",
    "    # TODO:[-] 25-06-12 屏蔽是会去掉该时刻的所有数据吗？\n",
    "    model.add(Masking(mask_value=0.0, input_shape=(61, 1)))\n",
    "    # step2: 添加双向LSTM层\n",
    "    # 双向 LSTM 同时从前向和后向处理时序数据，从而捕获更多上下文信息，提升特征提取能力。\n",
    "    # units=128：LSTM 层中每个方向上有 128 个神经元。\n",
    "    # return_sequences=True：输出每个时间步的结果，而非仅仅输出最后时刻的状态，这样可以将整个序列的信息传递到下一层。\n",
    "    # activation='relu'：将激活函数设置为 ReLU（而非 LSTM 默认的 tanh），可能有助于缓解梯度消失问题，不过这取决于具体任务。\n",
    "    # 注意：虽然此处指定了 input_shape=(25, 1)，但实际上在 Sequential 模型中第一层已经指定了输入形状，所以这里的 input_shape 参数可能是不必要或引起混淆（建议保持与 Masking 层一致，即 (61, 1)）。\n",
    "    # v1 激活函数:relu\n",
    "    # v2 改为: tanh\n",
    "    model.add(Bidirectional(LSTM(units=256, return_sequences=True,\n",
    "                                 activation='tanh',\n",
    "                                 input_shape=(61, 1))))\n",
    "    # step3: 添加 Dropout 层，在训练时随机将 20% 的神经元输出设为 0。\n",
    "    # Dropout 是一种正则化方法，有助于防止模型过拟合；通过随机丢弃部分神经元，模型不能过分依赖局部特征，从而提高泛化能力。\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Bidirectional(LSTM(units=128, return_sequences=True, activation='tanh')))  # 可以堆叠多个LSTM层\n",
    "    # step5: 再次添加一个 Dropout 层，使得第二层 LSTM 的输出在训练时有 20% 被随机置零，从而进一步防止过拟合。\n",
    "    model.add(Dropout(0.2))\n",
    "    # step4: 第二层双向LSTM层\n",
    "    model.add(Bidirectional(LSTM(units=64, return_sequences=True, activation='tanh')))  # 可以堆叠多个LSTM层\n",
    "    # step5: 再次添加一个 Dropout 层，使得第二层 LSTM 的输出在训练时有 20% 被随机置零，从而进一步防止过拟合。\n",
    "    model.add(Dropout(0.2))\n",
    "    # step6: 添加全连接层（Dense 层），输出节点数为 25。25-06-12 此处修改为 61 ，需要预测长度为61的时间向量\n",
    "    # Dense 层将前面的时序特征映射到目标空间。在时序网络中，当上层返回整个序列（形状为 [batch_size, time_steps, features]）时，Dense 层会被逐时步地应用，输出每个时间步对应一个长度为 25 的向量。这通常用于预测任务，比如多步预测或者每个时刻有多个目标值的任务。\n",
    "    model.add(Dense(61))\n",
    "\n",
    "    # 将均方误差修改为均方根误差后\n",
    "    # step7: 编译模型\n",
    "    # optimizer='adam' 使用 Adam 优化器进行梯度下降更新。Adam 优化器能够自适应调整各参数的学习率，通常能较快收敛，并且对超参数设定不太敏感，不同于传统的 SGD。\n",
    "    # TODO:[*] 25-06-12 什么是超参数？\n",
    "    # loss='mse' 将损失函数设为均方误差（Mean Squared Error），这是回归问题常用的误差度量指标，模型训练的目标就是尽可能使预测值与真实值之间的均方误差最小化。\n",
    "    # 整体作用 model.compile() 会对模型进行配置，指定训练时用哪个优化器、用哪个损失函数，如果需要，还可以添加额外的评估指标。编译过程会为模型建立必要的计算图，并对参数进行初始化。\n",
    "    model.compile(optimizer='adam', loss=lambda y_true, y_pred: weighted_mse(y_true, y_pred))\n",
    "\n",
    "    # step8: 训练模型\n",
    "    # X_train, y_train 分别为训练数据和对应目标值。模型将以这些数据为依据不断调整参数，使预测值与真实目标值之间的 MSE 最小化。\n",
    "    # epochs=10 指定训练过程需要遍历整个训练集 10 次。每个 epoch 内部数据会根据 batch 大小分批更新参数。\n",
    "    # batch_size=16 每个训练步骤（step）使用 16 个样本进行梯度计算和模型更新。较小的 batch size 有助于捕捉更多细微变化，但训练时间可能更长；较大的 batch size 则计算稳定但可能导致泛化性下降。\n",
    "    # validation_data=(X_test, y_test) 在每个 epoch 结束后，模型也会评估一次在验证集上的损失。这有助于监控过拟合情况以及训练过程的稳定性。\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_test, y_test))\n",
    "    model.save(model_path)\n",
    "\n",
    "    pass\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "def main():\n",
    "    to_do()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b68c69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
