## 二、数据标准化处理步骤

对应方法:

```python
create_samples
```



### 2-3 对特征`dataframe`进行标准化处理

```python
scaler = StandardScaler()
scaled_data = scaler.fit_transform(feature_df)
```



#### 1. 不会修改特征数据形状

`scaled_data` 的形状 (shape) 与 `feature_df` **完全相同**。

*   如果 `feature_df` 的形状是 `(10000, 40)`，即 10000 行（时间步）和 40 列（特征）。
*   那么 `scaled_data` 的形状也**会是 `(10000, 40)`**。

`StandardScaler` 是一个**逐列**进行的操作，它不会增加或删除任何行或列，只是改变了矩阵中每个元素的值。

唯一的微小区别是数据类型：`feature_df` 是一个 `pandas.DataFrame`，而 `scaler.fit_transform()` 返回的结果 `scaled_data` 是一个 `numpy.ndarray`。但它们的维度是完全一致的。

---

#### 2. 关于数值范围：这里有一个常见的误解。

`StandardScaler` **并不会**将数据分布在 `[0, 1]` 区间内。

将数据缩放到 `[0, 1]` 区间是另一种标准化方法，叫做 **`MinMaxScaler`** 的功能。

`StandardScaler` 做的是 **Z-score 标准化**，它的目标是：
*   使每一列的**均值变为 0**。
*   使每一列的**标准差变为 1**。

**这意味着：**
*   转换后的值可以是**正数**、**负数**或**零**。
*   一个原始值如果等于它所在列的均值，转换后就变成 `0`。
*   一个原始值如果比均值大一个标准差，转换后就变成 `+1`。
*   一个原始值如果比均值小两个标准差，转换后就变成 `-2`。

因此，`scaled_data` 的数值范围通常是在一个以0为中心的小区间内，比如 `[-3, 3]` （对于大多数数据点而言），但**理论上没有严格的上下界**，如果数据中有极端异常值，转换后的值可能会超出这个范围（例如 `5.0` 或 `-4.5`）。

#### 对比总结

为了让您更清晰地理解，这里是 `StandardScaler` 和 `MinMaxScaler` 的一个简单对比：

| 特性               | `StandardScaler` (您代码中使用的)             | `MinMaxScaler` (用于对比)                                    |
| :----------------- | :-------------------------------------------- | :----------------------------------------------------------- |
| **目标**           | 均值为 0，标准差为 1                          | 将数据缩放到一个给定的区间，默认为 `[0, 1]`                  |
| **公式**           | `z = (x - μ) / σ`                             | `x_scaled = (x - min) / (max - min)`                         |
| **结果分布**       | 数据分布在 0 附近                             | 数据被“压缩”或“拉伸”以适应 `[0, 1]` 区间                     |
| **结果范围**       | **没有严格边界**，但大部分数据在 `[-3, 3]` 内 | **严格在 `[0, 1]` 区间内**                                   |
| **对异常值敏感度** | 比较稳健，异常值不会对整体缩放产生过大影响    | 非常敏感，一个极大的异常值会把大部分正常数据“挤压”到一个很小的范围内 |

**结论：**
形状不变。
`StandardScaler` 是将数据中心化到均值为0，而不是缩放到 `[0, 1]` 区间。对于大多数神经网络模型，`StandardScaler` 是更常用、更稳健的选择。



### 2-4 标准化处理后的`StandardScaler` 由 n 组列向量组成

这正是从使用 Pandas 进行数据处理，到切换为使用 NumPy/TensorFlow/PyTorch 进行模型训练时，一个必须处理的核心环节。

1.  **`scaled_data` 缺少 `columns_name`** 。
2.  **我们需要通过额外的变量保存列名称**。
3.  **`scaled_data` 也缺少行坐标（对应的预报时间）**。

---

#### 1. 关于列名 (Column Names) 的处理

**问题：** `scaler.fit_transform(feature_df)` 返回的是一个 NumPy 数组 (`numpy.ndarray`)，它是一个纯粹的数值矩阵，不包含像 Pandas DataFrame 那样的列名 (`columns`) 信息。

**代码中的解决方案：**

在丢失列名信息之前，做了一个非常关键的操作：它将**列名转换为了列的整数索引**。

请看这部分代码：
```python
# 2.3 定义编码器、解码器和目标的特征列
# ... (省略了特征列表的创建) ...

# 获取这些特征在 scaled_data 中的列索引
df_cols = feature_df.columns.tolist() # <--- 在这里获取了所有列名的列表
encoder_indices = [df_cols.index(col) for col in encoder_features]
decoder_indices = [df_cols.index(col) for col in decoder_features]
target_indices = [df_cols.index(col) for col in target_features]
```

**工作流程如下：**

1.  `df_cols = feature_df.columns.tolist()`：将 `feature_df` 的所有列名保存为一个列表，例如 `['MF01002_real_u', 'MF01002_real_v', ...]`。这个列表的顺序与 `scaled_data` 的列顺序是**完全一致的**。
2.  `df_cols.index(col)`：对于每一个我们需要的特征（比如 `'MF01002_real_u'`），代码查找它在 `df_cols` 列表中的位置（索引）。例如，如果它是第一个特征，它的索引就是 `0`。
3.  `encoder_indices`, `decoder_indices`, `target_indices`：这三个变量最终保存的不是特征的字符串名字，而是它们在 `scaled_data` 这个 NumPy 数组里对应的**列号**。

所以，当后续代码执行 `scaled_data[start:end, encoder_indices]` 时，它能够精确地从没有列名的 `scaled_data` 数组中，抽取出所有编码器所需要的特征列。

此外，函数的最后也明确返回了列名，以备后续分析之需：
```python
return encoder_x, decoder_x, target_y, scaler, feature_df.columns
```

---

#### 2. 关于行坐标 (预报时间) 的处理

**问题：** 与列名一样，`scaled_data` 也丢失了 `feature_df` 的行索引 (`index`)，也就是“对应的预报时间”。

**代码中的解决方案：**

时间信息在这里是通过**隐式的方式**，也就是**数据的顺序**来保留的。

1.  **初始数据有序**：`feature_df` 的索引（`index`）是从 `merged_df` 继承而来的，它是一个 `DatetimeIndex`，并且是按时间先后顺序排列的。
2.  **转换过程保持顺序**：`scaler.fit_transform()` 操作不会打乱行的顺序。因此，`scaled_data` 的第 `i` 行数据就对应于 `feature_df` 的第 `i` 个时间点。
3.  **滑动窗口顺序切分**：代码中的 `for i in range(...)` 循环是**按顺序**从头到尾遍历 `scaled_data` 的。它创建的第一个样本 (`i=0`) 对应的是最早的时间窗口，第二个样本 (`i=1`) 对应的时间窗口向后推移了一步，以此类推。

**结论就是：**
我们不需要为 `scaled_data` 的每一行都附上一个时间戳。因为整个数组的行顺序本身就代表了时间的流逝。模型（尤其是像 LSTM、GRU 这样的序列模型）正是依赖于输入序列的这种**先后顺序**来学习时间依赖性的。

---

#### 总结一下工作流程：

1.  **Pandas `DataFrame` (feature_df)**
    *   **优点**：数据结构丰富，带有关联信息（列名、时间索引），便于理解和操作。
    *   **阶段**：特征工程阶段。

2.  **NumPy `ndarray` (scaled_data)**
    *   **优点**：纯数值，内存占用更小，计算效率极高，是深度学习框架的理想输入格式。
    *   **“缺点”**：丢失了元数据（列名、索引）。
    *   **阶段**：从数据准备到模型输入的过渡阶段。

**利用 Pandas 完成复杂的、需要标签对齐的特征工程，然后在最后一步将其转换为高效的 NumPy 数组，并在此之前将所需的“元数据”（如列名）转换为“索引”，以供后续的数组切片使用。**

它揭示了在实践中从 Pandas 到 NumPy/TensorFlow/PyTorch 进行数据转换时的核心操作和注意事项。