{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import arrow\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "# TODO:[-] 25-06-03 keras.src 路径是 TensorFlow 2.11 及更高版本中集成在 TensorFlow 内部的 Keras 3 中使用的。\n",
    "# from keras.src.layers import LSTM, Dropout, Bidirectional, Dense, Masking\n",
    "from keras.layers import LSTM, Dropout, Bidirectional, Dense, Masking\n",
    "from pandas import DatetimeIndex\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# from tensorflow.keras.losses import MeanSquaredError\n",
    "# 可视化结果（如果需要）\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pathlib\n",
    "import xml.etree.ElementTree as ET\n",
    "import xarray as xr\n",
    "import codecs\n",
    "import datetime\n",
    "\n",
    "# 先从海浪数据中提取出经纬度，时间，风，海浪高度\n",
    "# 解析单个文件，并存于字典内\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "38af8939",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_path: str = r'Z:\\SOURCE_MERGE_DATA\\df_ws_forecast.csv'\n",
    "realdata_path: str = r'Z:\\SOURCE_MERGE_DATA\\2024_local_df_utc_183_split.csv'\n",
    "'''按照三小时一个提取的实况数据路径'''\n",
    "model_path: str = r'Z:\\02TRAINNING_MODEL\\fit_model_v4_250617.h5'\n",
    "# scaler_forecast: str = r'Z:\\01TRAINNING_DATA\\scaler\\scaler_forecast_250609.sav'\n",
    "# scaler_realdata: str = r'Z:\\01TRAINNING_DATA\\scaler\\scaler_realdata_250609.sav'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d237633",
   "metadata": {},
   "source": [
    "#### 新增修改，只提取[0,24]的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2f8fb56d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # step1: 加载标准化后的 预报 | 实况 数据集\n",
    "# shape: (61,732)\n",
    "df_forecast = pd.read_csv(forecast_path, encoding='utf-8', index_col=0)\n",
    "# shape:(61,731)\n",
    "df_realdata = pd.read_csv(realdata_path, encoding='utf-8', index_col=0)\n",
    "# 实况拼接有问题需要手动去掉最后一列\n",
    "# 使用按3小时进行分割的数据不需要去掉最后一列，这样 realdata 与 forecast 的 columns 一致\n",
    "# df_realdata = df_realdata.drop(df_realdata.columns[-1], axis=1)\n",
    "df_forecast = df_forecast.iloc[:61, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0dd68e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61, 732)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a43dd582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 732)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_forecast_24=df_forecast.iloc[0:8,:]\n",
    "df_forecast_24.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "419d58cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 732)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_realdata_24= df_realdata.iloc[0:8,:]\n",
    "df_realdata_24.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc4ddcc",
   "metadata": {},
   "source": [
    "#### step2: 将[0,24]的数据作为训练及预测数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0eb80b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step3: 数据标准化(提出nan值)\n",
    "rows: int = df_forecast_24.shape[0]\n",
    "cols: int = df_realdata_24.shape[1]\n",
    "# TODO:[-] 25-05-28 注意原始数据中: forecast (61,732), real (61,732)\n",
    "X = df_forecast_24.values.T.reshape(cols, rows, 1)\n",
    "# TODO:[*] 25-05-11 注意 y 中有存在 nan\n",
    "# ValueError: cannot reshape array of size 52776 into shape (732,72,1)\n",
    "y = df_realdata_24.values.T.reshape(cols, rows, 1)\n",
    "# step3-2:对数据进行归一化\n",
    "# 拍扁数据为二维数组（n*timesteps, feature）进行归一化\n",
    "X_flat = X.reshape(-1, 1)\n",
    "y_flat = y.reshape(-1, 1)\n",
    "# 分别为 X 和 y 定义归一化器（当然如果两者量纲一致，可用同一个 scaler）\n",
    "scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = scaler_X.fit_transform(X_flat)\n",
    "y_scaled = scaler_y.fit_transform(y_flat)\n",
    "# TODO:[-] 25-06-09 保存归一化器\n",
    "# joblib.dump(scaler_X, scaler_forecast)\n",
    "# joblib.dump(scaler_y, scaler_realdata)\n",
    "# 将归一化后的二维数据恢复为原来的3D形状\n",
    "X = X_scaled.reshape(X.shape)\n",
    "y = y_scaled.reshape(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e84870d",
   "metadata": {},
   "source": [
    "`X`为特征或输入数据，这是模型用来进行预测的依据。  \n",
    "`y`: 通常是一个数组或Series，代表了你的标签 (Labels) 或 目标变量 (Target)。这是你希望模型学会预测的“正确答案”。  \n",
    "在这里`X`就是原始风场的预报结果;`y`就是实况数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb883f5",
   "metadata": {},
   "source": [
    "这是一个非常重要的参数，用于控制数据的随机性。在分割数据之前，函数会先将数据进行随机打乱，以确保训练集和测试集中的数据分布是相似的。  \n",
    "random_state 相当于为这个随机过程设定一个**“种子”。只要这个数字（这里是 42）不变，那么每次运行这段代码，你得到的分割结果都将是完全一样的**。  \n",
    "这对于保证实验的可复现性至关重要。如果你想和同事分享你的代码，或者自己重复实验来比较不同模型的效果，设置 random_state 可以确保你们都是在完全相同的数据分割上进行操作。数字 42 本身没有特殊含义，任何整数都可以，它只是一个社区中常用的约定。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7c0d6a",
   "metadata": {},
   "source": [
    "| 变量名  | 包含内容           | 作用                          | 占比  |\n",
    "|--------|------------------|-----------------------------|------|\n",
    "| X_train | 特征 (Features)  | 用于训练模型                | 80%  |\n",
    "| y_train | 标签 (Labels)    | 作为训练时的标准答案        | 80%  |\n",
    "| X_test  | 特征 (Features)  | 用于测试模型，模型在训练时未见过 | 20%  |\n",
    "| y_test  | 标签 (Labels)    | 作为测试时的评分依据，模型在训练时未见过 | 20%  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd54884",
   "metadata": {},
   "source": [
    "## 一个形象的比喻\n",
    "\n",
    "你可以把这个过程想象成一个学生准备考试：\n",
    "\n",
    "- **X_train 和 y_train** 是 **教科书** 和 **所有课后练习题（及答案）**。学生（模型）通过学习这些材料来掌握知识。\n",
    "- **X_test** 是 **期末考试的试卷（只有题目）**。这些题目是学生以前从未见过的。\n",
    "- **y_test** 是 **老师手里的标准答案**。老师用它来批改学生的考卷，并给出最终分数。\n",
    "\n",
    "这个分割过程确保了我们能公正地评估模型是否真正“学到”了知识，而不是仅仅“背下”了练习题。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0a249ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f823a195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((585, 8, 1), (585, 8, 1), (147, 8, 1), (147, 8, 1))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63291c53",
   "metadata": {},
   "source": [
    "##### step3: 数据标准化(剔除奇异值)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1f0efcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train, dtype=np.float32)\n",
    "X_test = np.array(X_test, dtype=np.float32)\n",
    "y_train = np.array(y_train, dtype=np.float32)\n",
    "y_test = np.array(y_test, dtype=np.float32)\n",
    "# step4: 构建模型\n",
    "X_train = np.nan_to_num(X_train, nan=0.0)\n",
    "X_test = np.nan_to_num(X_test, nan=0.0)\n",
    "y_train = np.nan_to_num(y_train, nan=0.0)\n",
    "y_test = np.nan_to_num(y_test, nan=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b32516",
   "metadata": {},
   "source": [
    "#### step4: 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "746edacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path=r'Z:\\02TRAINNING_MODEL\\fit_model_v3.1_24_250618.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "15593fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "37/37 [==============================] - 22s 202ms/step - loss: 0.0255 - val_loss: 0.0171\n",
      "Epoch 2/10\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 0.0174 - val_loss: 0.0158\n",
      "Epoch 3/10\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 0.0156 - val_loss: 0.0150\n",
      "Epoch 4/10\n",
      "37/37 [==============================] - 3s 73ms/step - loss: 0.0151 - val_loss: 0.0150\n",
      "Epoch 5/10\n",
      "37/37 [==============================] - 3s 79ms/step - loss: 0.0148 - val_loss: 0.0159\n",
      "Epoch 6/10\n",
      "37/37 [==============================] - 3s 84ms/step - loss: 0.0143 - val_loss: 0.0150\n",
      "Epoch 7/10\n",
      "37/37 [==============================] - 3s 78ms/step - loss: 0.0145 - val_loss: 0.0158\n",
      "Epoch 8/10\n",
      "37/37 [==============================] - 3s 74ms/step - loss: 0.0141 - val_loss: 0.0148\n",
      "Epoch 9/10\n",
      "37/37 [==============================] - 3s 82ms/step - loss: 0.0138 - val_loss: 0.0150\n",
      "Epoch 10/10\n",
      "37/37 [==============================] - 3s 83ms/step - loss: 0.0142 - val_loss: 0.0150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c8dadc3fd0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO:[-] 25-06-12 屏蔽是会去掉该时刻的所有数据吗？\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0.0, input_shape=(8, 1)))\n",
    "# step2: 添加双向LSTM层\n",
    "# 双向 LSTM 同时从前向和后向处理时序数据，从而捕获更多上下文信息，提升特征提取能力。\n",
    "# units=128：LSTM 层中每个方向上有 128 个神经元。\n",
    "# return_sequences=True：输出每个时间步的结果，而非仅仅输出最后时刻的状态，这样可以将整个序列的信息传递到下一层。\n",
    "# activation='relu'：将激活函数设置为 ReLU（而非 LSTM 默认的 tanh），可能有助于缓解梯度消失问题，不过这取决于具体任务。\n",
    "# 注意：虽然此处指定了 input_shape=(25, 1)，但实际上在 Sequential 模型中第一层已经指定了输入形状，所以这里的 input_shape 参数可能是不必要或引起混淆（建议保持与 Masking 层一致，即 (61, 1)）。\n",
    "# v1 激活函数:relu\n",
    "# v2 改为: tanh\n",
    "model.add(Bidirectional(LSTM(units=256, return_sequences=True,\n",
    "                             activation='tanh',\n",
    "                             input_shape=(8, 1))))\n",
    "# step3: 添加 Dropout 层，在训练时随机将 20% 的神经元输出设为 0。\n",
    "# Dropout 是一种正则化方法，有助于防止模型过拟合；通过随机丢弃部分神经元，模型不能过分依赖局部特征，从而提高泛化能力。\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(units=128, return_sequences=True, activation='tanh')))  # 可以堆叠多个LSTM层\n",
    "# step5: 再次添加一个 Dropout 层，使得第二层 LSTM 的输出在训练时有 20% 被随机置零，从而进一步防止过拟合。\n",
    "model.add(Dropout(0.2))\n",
    "# step4: 第二层双向LSTM层\n",
    "model.add(Bidirectional(LSTM(units=64, return_sequences=True, activation='tanh')))  # 可以堆叠多个LSTM层\n",
    "# step5: 再次添加一个 Dropout 层，使得第二层 LSTM 的输出在训练时有 20% 被随机置零，从而进一步防止过拟合。\n",
    "model.add(Dropout(0.2))\n",
    "# step6: 添加全连接层（Dense 层），输出节点数为 25。25-06-12 此处修改为 61 ，需要预测长度为61的时间向量\n",
    "# Dense 层将前面的时序特征映射到目标空间。在时序网络中，当上层返回整个序列（形状为 [batch_size, time_steps, features]）时，Dense 层会被逐时步地应用，输出每个时间步对应一个长度为 25 的向量。这通常用于预测任务，比如多步预测或者每个时刻有多个目标值的任务。\n",
    "# TODO:[*] 25-06-17 此处将最后一个全链接层节点数由 25 -> 1\n",
    "model.add(Dense(1))\n",
    "# 将均方误差修改为均方根误差后\n",
    "# step7: 编译模型\n",
    "# optimizer='adam' 使用 Adam 优化器进行梯度下降更新。Adam 优化器能够自适应调整各参数的学习率，通常能较快收敛，并且对超参数设定不太敏感，不同于传统的 SGD。\n",
    "# TODO:[*] 25-06-12 什么是超参数？\n",
    "# loss='mse' 将损失函数设为均方误差（Mean Squared Error），这是回归问题常用的误差度量指标，模型训练的目标就是尽可能使预测值与真实值之间的均方误差最小化。\n",
    "# 整体作用 model.compile() 会对模型进行配置，指定训练时用哪个优化器、用哪个损失函数，如果需要，还可以添加额外的评估指标。编译过程会为模型建立必要的计算图，并对参数进行初始化。\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# ---------------------------------------------\n",
    "# step8: 训练模型\n",
    "# X_train, y_train 分别为训练数据和对应目标值。模型将以这些数据为依据不断调整参数，使预测值与真实目标值之间的 MSE 最小化。\n",
    "# epochs=10 指定训练过程需要遍历整个训练集 10 次。每个 epoch 内部数据会根据 batch 大小分批更新参数。\n",
    "# batch_size=16 每个训练步骤（step）使用 16 个样本进行梯度计算和模型更新。较小的 batch size 有助于捕捉更多细微变化，但训练时间可能更长；较大的 batch size 则计算稳定但可能导致泛化性下降。\n",
    "# validation_data=(X_test, y_test) 在每个 epoch 结束后，模型也会评估一次在验证集上的损失。这有助于监控过拟合情况以及训练过程的稳定性。\n",
    "# 将 validation_data 从 (X_test, y_test) 修改为 (X_test, y_test, val_weights)\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_test, y_test))\n",
    "# model.save(model_path)s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db07906",
   "metadata": {},
   "source": [
    "#### step5: 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b94d20fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((147, 8, 1), (147, 8, 1))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527231a5",
   "metadata": {},
   "source": [
    "* 注意此处存在一个隐藏的bug，此处不能使用`X_test`是作为训练时的对比的真实数据，应该采用`y_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49774064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2282af1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147, 8, 1)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "be650c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 82ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "be25ce50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147, 8, 1)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee5b625",
   "metadata": {},
   "source": [
    " 1. 使用 scaler_y 来进行反归一化，因为 y_pred 是对 y_scaled 的预测\n",
    " 2. 修改变量名以反映其真实含义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "522ada7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_denormalize=scaler_y.inverse_transform(y_pred.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5f598",
   "metadata": {},
   "source": [
    "3. 将反归一化后的数据恢复为与 y_test 相同的3D形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "41339837",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_denormalize = y_pred_denormalize.reshape(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e2fd02ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147, 8, 1)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_denormalize.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627bbe41",
   "metadata": {},
   "source": [
    "* 注意numpy.ndarray可以通过 `arr[:,:,x]` 进行切分，类似于 `df.iloc[:,:,x]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8e372b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_finally=y_pred_denormalize[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ab9abb5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147, 8)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_denormalize=scaler_X.inverse_transform(X_test.reshape(-1, 1))\n",
    "x_test_denormalize = x_test_denormalize.reshape(X_test.shape)\n",
    "x_test_denormalize=x_test_denormalize[:,:,0]\n",
    "x_test_denormalize.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c697cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(x_series: pd.Series, y_series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "        计算 x_series 与 y_series 的RMSE\n",
    "    :param x_series:\n",
    "    :param y_series:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 计算差值的平方\n",
    "    squared_errors = (x_series - y_series) ** 2\n",
    "\n",
    "    # 计算每列的均方误差 (MSE)\n",
    "    mse_per_column = squared_errors.mean(axis=0)  # axis=0 表示按列计算均值\n",
    "\n",
    "    # 计算每列的均方根误差 (RMSE)\n",
    "    rmse = np.sqrt(mse_per_column)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e7fc1136",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_series = compute_rmse(y_pred_finally, x_test_denormalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9a4bc5bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.4727689 , 1.2196039 , 1.07165   , 0.97771233, 1.1183887 ,\n",
       "       1.3672862 , 1.4856521 , 1.449054  ], dtype=float32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "82803f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rmse_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24095f96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
